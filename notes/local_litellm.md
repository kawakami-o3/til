# ãƒ­ãƒ¼ã‚«ãƒ«AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™ºç’°å¢ƒ (LiteLLM + Ollama) ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

Docker Desktopã‚’ä½¿ç”¨ã—ã¦ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã«LLMãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
ä»®æƒ³ã‚­ãƒ¼ï¼ˆèªè¨¼ï¼‰ã¯ä½¿ç”¨ã›ãšã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚­ã‚·æ§‹æˆã¨ã—ã¾ã™ã€‚
ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•æ™‚ã«Ollamaã‚µãƒ¼ãƒãƒ¼ãŒç«‹ã¡ä¸ŠãŒã‚Šã€æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

## 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¾ã™ã€‚

```text
my-agent-env/
â”œâ”€â”€ docker-compose.yml     # æ§‹æˆå®šç¾©
â”œâ”€â”€ litellm_config.yaml    # ãƒ¢ãƒ‡ãƒ«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®š
â””â”€â”€ entrypoint.sh          # Ollamaèµ·å‹•ãƒ»è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```

---

## 2. ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

### entrypoint.sh
Ollamaã®èµ·å‹•ã‚’å¾…ã¡ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯è‡ªå‹•ã§Pullã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
ãƒãƒ¼ãƒªãƒ³ã‚°å‡¦ç†ã‚’å…¥ã‚Œã¦ã„ã‚‹ãŸã‚ã€èµ·å‹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ã‚ˆã‚‹æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚’é˜²ãã¾ã™ã€‚

```bash
#!/bin/bash

# 1. Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
/bin/ollama serve &

# ãƒ—ãƒ­ã‚»ã‚¹IDã‚’å–å¾—
pid=$!

echo "â³ Waiting for Ollama server to start..."

# 2. ãƒãƒ¼ãƒªãƒ³ã‚°å‡¦ç†
# localhost:11434 ã«æ¥ç¶šã§ãã‚‹ã¾ã§å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—ï¼‰
while ! (echo > /dev/tcp/localhost/11434) >/dev/null 2>&1; do
    sleep 1
done

echo "âœ… Ollama server is active!"

# 3. ãƒ¢ãƒ‡ãƒ«ã®å–å¾—å‡¦ç†
echo "ğŸ”´ Checking model: ${OLLAMA_MODEL}..."
ollama pull ${OLLAMA_MODEL}
echo "ğŸŸ¢ Model ${OLLAMA_MODEL} is ready!"

# 4. ã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ãªã„ã‚ˆã†ã«å¾…æ©Ÿã—ç¶šã‘ã‚‹
wait $pid
```

### litellm_config.yaml
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’Ollamaã‚³ãƒ³ãƒ†ãƒŠã¸è»¢é€ã™ã‚‹è¨­å®šã§ã™ã€‚
`api_base` ã«ã¯Dockerã‚³ãƒ³ãƒ†ãƒŠåï¼ˆ`ollama-container`ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚

```yaml
model_list:
  - model_name: local-gpt   # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰æŒ‡å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
    litellm_params:
      model: ollama/llama3.2:3b  # å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«
      api_base: http://ollama-container:11434
```

### docker-compose.yml
2ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆOllama, LiteLLMï¼‰ã‚’å®šç¾©ã—ã¾ã™ã€‚
`OLLAMA_HOST=0.0.0.0` ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ã‚³ãƒ³ãƒ†ãƒŠå¤–ï¼ˆLiteLLMï¼‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¾ã™ã€‚

```yaml
services:
  # 1. ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¨ãƒ³ã‚¸ãƒ³ (Ollama)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
      - ./entrypoint.sh:/entrypoint.sh
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    environment:
      - OLLAMA_MODEL=llama3.2:3b  # ã“ã“ã§ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
      - OLLAMA_HOST=0.0.0.0       # å¤–éƒ¨æ¥ç¶šè¨±å¯ï¼ˆå¿…é ˆï¼‰

  # 2. ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒ (LiteLLM)
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-backend
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    # LITELLM_MASTER_KEY ã¯è¨­å®šã—ãªã„ï¼ˆèªè¨¼ãªã—ãƒ¢ãƒ¼ãƒ‰ï¼‰
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
    depends_on:
      - ollama

volumes:
  ollama_storage:
```

---

## 3. èµ·å‹•æ‰‹é †

ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
# 1. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸ï¼ˆåˆå›ã®ã¿å¿…è¦ï¼‰
chmod +x entrypoint.sh

# 2. ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•
docker-compose up -d
```

åˆå›èµ·å‹•æ™‚ã¯ãƒ¢ãƒ‡ãƒ«ï¼ˆLlama 3.2ãªã©ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚ã€æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§é€²æ—ã‚’ç¢ºèªã§ãã¾ã™ã€‚
```bash
docker logs -f ollama-container
```
`Model llama3.2:3b is ready!` ã¨è¡¨ç¤ºã•ã‚Œã‚Œã°æº–å‚™å®Œäº†ã§ã™ã€‚

---

## 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚³ãƒ¼ãƒ‰å®Ÿè£…ä¾‹ (Python)

OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã—ã¦å®Ÿè£…ã—ã¾ã™ã€‚APIã‚­ãƒ¼ã¯ãƒ€ãƒŸãƒ¼æ–‡å­—åˆ—ã§å‹•ä½œã—ã¾ã™ã€‚

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-dummy",              # LiteLLM (No Auth) ãªã®ã§ä½•ã§ã‚‚OK
    base_url="http://localhost:4000" # ãƒ­ãƒ¼ã‚«ãƒ«ã®LiteLLMã«å‘ã‘ã‚‹
)

try:
    response = client.chat.completions.create(
        model="local-gpt",           # configã§å®šç¾©ã—ãŸåå‰
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯ã€ãƒ†ã‚¹ãƒˆä¸­ã§ã™ã‹ï¼Ÿ"}
        ]
    )
    print(response.choices[0].message.content)

except Exception as e:
    print(f"Error: {e}")
```

## 5. é‹ç”¨ã‚³ãƒãƒ³ãƒ‰

- **åœæ­¢:** `docker-compose down`
- **ãƒ¢ãƒ‡ãƒ«å¤‰æ›´:** `docker-compose.yml` ã® `OLLAMA_MODEL` ã‚’æ›¸ãæ›ãˆã¦ `docker-compose up -d`
- **ãƒ­ã‚°ç¢ºèª:** `docker-compose logs -f`

